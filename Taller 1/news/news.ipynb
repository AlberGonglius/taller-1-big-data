{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54231597",
   "metadata": {},
   "source": [
    "## 1. Configuración de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e73f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\n",
    "    \"PYSPARK_SUBMIT_ARGS\"\n",
    "] = \"--packages org.apache.hadoop:hadoop-aws:3.2.2,io.delta:delta-core_2.12:1.1.0  pyspark-shell \"\n",
    "config = {\n",
    "    \"spark.jars.packages\":\"org.apache.hadoop:hadoop-aws:3.2.2\",\n",
    "    \"spark.kubernetes.namespace\": \"spark\",\n",
    "    \"spark.kubernetes.container.image\": \"cronosnull/abd-spark-base:202301\",\n",
    "    \"spark.executor.instances\": \"4\",\n",
    "    \"spark.executor.memory\": \"1g\",\n",
    "    \"spark.executor.cores\": \"1\",\n",
    "    \"spark.driver.port\":\"38889\",\n",
    "    \"spark.driver.blockManager.port\":\"7777\",\n",
    "    \"spark.driver.bindAddress\": \"0.0.0.0\",\n",
    "    \"spark.driver.host\":\"172.24.99.30\",\n",
    "    \"spark.kubernetes.executor.request.cores\":\"500m\",\n",
    "    \"spark.driver.memory\":\"4g\",\n",
    "    \"spark.hadoop.fs.s3a.endpoint\": \"http://172.24.99.18:9000\",\n",
    "    \"spark.hadoop.fs.s3a.access.key\": os.environ.get('MINIO_USERNAME', \"--\"),\n",
    "    \"spark.hadoop.fs.s3a.secret.key\": os.environ.get('MINIO_PASSWORD',\"--\"),\n",
    "    \"spark.hadoop.fs.s3a.path.style.access\": True,\n",
    "    \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "    \"spark.hadoop.fs.s3a.aws.credentials.provider\": \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    "    \"spark.kubernetes.local.dirs.tmpfs\":True,\n",
    "\n",
    "}\n",
    "def get_spark_session(app_name: str, conf: SparkConf):\n",
    "    conf.setMaster(\"k8s://https://172.24.99.68:16443\")\n",
    "    for key, value in config.items():\n",
    "        conf.set(key, value)    \n",
    "    return SparkSession.builder.appName(app_name).config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2699e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get or Create Spark session\n",
    "spark = get_spark_session(\"grupo03-news\", SparkConf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "028adeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\r\n",
      "      ____              __\r\n",
      "     / __/__  ___ _____/ /__\r\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\r\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.2.0\r\n",
      "      /_/\r\n",
      "                        \r\n",
      "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 1.8.0_322\r\n",
      "Branch \r\n",
      "Compiled by user  on 2022-03-26T09:34:47Z\r\n",
      "Revision \r\n",
      "Url \r\n",
      "Type --help for more information.\r\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d4f06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.24.99.30:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>k8s://https://172.24.99.68:16443</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>grupo03-news</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcd081c4430>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daa59cd",
   "metadata": {},
   "source": [
    "## 2. Archivo de Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "568ec0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading stop words\n",
    "#stopwords_df = spark.read.text(\"data/userdata/grupo-03/stopwords/spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a5ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading stop words\n",
    "import requests\n",
    "response = requests.get('https://github.com/bsgarciac/taller1-bigdata/blob/master/data/spanish.txt')\n",
    "stopwords = response.json()['payload']['blob']['rawLines']\n",
    "data = [(string,) for string in stopwords] # fixing schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc081c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding more Stopwords\n",
    "data.append((\"tras\", ))\n",
    "data.append((\"año\", ))\n",
    "data.append((\"años\", ))\n",
    "data.append((\"según\", ))\n",
    "data.append((\"así\", ))\n",
    "data.append((\"ademas\", ))\n",
    "data.append((\"hace\", ))\n",
    "data.append((\"ser\", ))\n",
    "data.append((\"pide\", ))\n",
    "data.append((\"parte\", ))\n",
    "data.append((\"puede\", ))\n",
    "data.append((\"ser\", ))\n",
    "data.append((\"sido\", ))\n",
    "data.append((\"después\", ))\n",
    "data.append((\"además\", ))\n",
    "data.append((\"si\", ))\n",
    "# Stop words to DF\n",
    "stopwords_df = spark.createDataFrame(data, [\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de557ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords dataframe to String\n",
    "stopwords_grouped_df = stopwords_df.groupBy().agg(\n",
    "            f.concat_ws(\" \", f.collect_set(\"value\"))\n",
    "        )\n",
    "stopwords_str = stopwords_grouped_df.collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924c41ae",
   "metadata": {},
   "source": [
    "## 3. Archivos de Noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15e2c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading news Data schema\n",
    "news_df = spark.read.json(\"s3a://noticias2016/individual_files/news_0000001.json.gz\")\n",
    "schema = news_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6068714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 0:04:49.346136\n"
     ]
    }
   ],
   "source": [
    "# Loading news data\n",
    "start = datetime.now()\n",
    "news_df = spark.read.json(\"s3a://noticias2016/individual_files/*.json.gz\", schema=schema)\n",
    "end = datetime.now()\n",
    "print(\"Execution Time:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eae37451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------\n",
      " author         |                      \n",
      " crawled        | 2016-10-20T23:44:... \n",
      " entities       | {[], [], []}         \n",
      " external_links | []                   \n",
      " highlightText  |                      \n",
      " highlightTitle |                      \n",
      " language       | spanish              \n",
      " locations      | []                   \n",
      " ord_in_thread  | 0                    \n",
      " organizations  | []                   \n",
      " persons        | []                   \n",
      " published      | 2016-10-20T14:36:... \n",
      " text           | MundoDeportivo.co... \n",
      " thread         | {ES, 2564, http:/... \n",
      " title          | Los rumores de fi... \n",
      " url            | http://www.mundod... \n",
      " uuid           | 96eb6858709dc9ffb... \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing a sample for news DF\n",
    "news_df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a29f0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[stopwords: array<string>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding stopwords to News dataframe\n",
    "news_df = news_df.withColumn(\"stopwords\", f.lit(stopwords_str))\n",
    "news_df = news_df.withColumn(\"stopwords\", f.split(\"stopwords\", ' '))\n",
    "news_df.select('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d8598a",
   "metadata": {},
   "source": [
    "## 4. Top 10 palabras más frecuentes en los títulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bae6c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting title\n",
    "news_df = news_df.withColumn(\"title_standarized\", f.lower('title'))\n",
    "news_df = news_df.withColumn(\"title_standarized\", f.regexp_replace('title_standarized', \"-|\\t|\\n|\\.|,|\\|\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "770f7812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing words in news title\n",
    "news_df = news_df.withColumn(\"words_title\", f.split('title_standarized', ' '))\n",
    "# Removing stopwords from words in title\n",
    "news_df = news_df.withColumn(\"words_title_filtered\", f.array_except(\"words_title\", \"stopwords\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bdb18519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words frequency in Title\n",
    "title_count_df = news_df.withColumn('word', f.explode('words_title_filtered')) \\\n",
    "    .groupBy('word').count() \\\n",
    "    .sort('count', ascending=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bb5761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       word|count|\n",
      "+-----------+-----+\n",
      "|           |58020|\n",
      "|     diesel|10706|\n",
      "|     madrid| 9201|\n",
      "|ecodiarioes| 9048|\n",
      "|      nuevo| 7202|\n",
      "|      color| 6564|\n",
      "|       2016| 6469|\n",
      "|   gobierno| 5991|\n",
      "|        dos| 5699|\n",
      "|     españa| 5558|\n",
      "|        4x4| 5485|\n",
      "|       psoe| 5187|\n",
      "|   millones| 5146|\n",
      "|  barcelona| 4796|\n",
      "|      trump| 4257|\n",
      "|    octubre| 4202|\n",
      "|  univision| 3983|\n",
      "|      nueva| 3791|\n",
      "|         20| 3758|\n",
      "|   gasolina| 3649|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing title count df\n",
    "title_count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51120ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 0:05:52.349614\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "title_count_df.write.parquet(\"s3a://user-data/grupo-03/title_count\")\n",
    "end = datetime.now()\n",
    "print(\"Execution Time:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a086c6",
   "metadata": {},
   "source": [
    "## 5. Top 10 palabras más frecuentes en el contenido de las noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8780e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making all words in title lower case\n",
    "news_df = news_df.withColumn(\"text_standarized\", f.lower('text'))\n",
    "news_df = news_df.withColumn(\"text_standarized\", f.regexp_replace('text_standarized', \"-|\\t|\\n|\\.|,|\\|\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ef7c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing words in news text\n",
    "news_df = news_df.withColumn(\"words_text\", f.split('text_standarized', ' '))\n",
    "# Removing stopwords from words in text\n",
    "news_df = news_df.withColumn(\"words_text_filtered\", f.array_except(\"words_text\", \"stopwords\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db35ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words frequency in Text\n",
    "text_count_df = news_df.withColumn('word', f.explode('words_text_filtered')) \\\n",
    "    .groupBy('word').count() \\\n",
    "    .sort('count', ascending=False).limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33f2c01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      word| count|\n",
      "+----------+------+\n",
      "|          |262205|\n",
      "|       dos|108303|\n",
      "|      2016| 99768|\n",
      "|   octubre| 90702|\n",
      "|      tres| 67132|\n",
      "|       vez| 66328|\n",
      "|    pasado| 65812|\n",
      "|  gobierno| 63240|\n",
      "|     ahora| 62809|\n",
      "|       hoy| 61018|\n",
      "|   primera| 60776|\n",
      "|  personas| 60454|\n",
      "|       día| 59645|\n",
      "|presidente| 59282|\n",
      "|  nacional| 58598|\n",
      "|      gran| 57621|\n",
      "|      país| 56919|\n",
      "|      cada| 56219|\n",
      "|     hacer| 55774|\n",
      "|      dijo| 55686|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time: 0:18:18.295750\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "text_count_df.show()\n",
    "end = datetime.now()\n",
    "print(\"Execution Time:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "197df78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 0:17:28.459716\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "text_count_df.write.parquet(\"s3a://user-data/grupo-03/news/text_count\")\n",
    "end = datetime.now()\n",
    "print(\"Execution Time:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af87a350",
   "metadata": {},
   "source": [
    "## 6. Finalizando sesión de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9ae5645",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1676b31",
   "metadata": {},
   "source": [
    "## 7. Análisis Comparativo\n",
    "\n",
    "### 7.1 Primera Prueba: 1 Nodo Time Out\n",
    "\n",
    "Inicialmente se intentó ejecutar este procedimiento con la siguiente configuración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa340ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_conf = {\n",
    "    \"spark.executor.instances\": \"1\",\n",
    "    \"spark.executor.memory\": \"1g\",\n",
    "    \"spark.executor.cores\": \"1\",\n",
    "    \"spark.driver.memory\":\"1g\",\n",
    "\n",
    "}\n",
    "config = config | first_conf\n",
    "config_df = pd.DataFrame.from_dict(config, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "118c3f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spark.jars.packages</th>\n",
       "      <td>org.apache.hadoop:hadoop-aws:3.2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.kubernetes.namespace</th>\n",
       "      <td>spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.kubernetes.container.image</th>\n",
       "      <td>cronosnull/abd-spark-base:202301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.executor.instances</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.executor.memory</th>\n",
       "      <td>1g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.executor.cores</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.driver.port</th>\n",
       "      <td>38889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.driver.blockManager.port</th>\n",
       "      <td>7777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.driver.bindAddress</th>\n",
       "      <td>0.0.0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.driver.host</th>\n",
       "      <td>172.24.99.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.kubernetes.executor.request.cores</th>\n",
       "      <td>500m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.driver.memory</th>\n",
       "      <td>1g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.endpoint</th>\n",
       "      <td>http://172.24.99.18:9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.access.key</th>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.secret.key</th>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.path.style.access</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.impl</th>\n",
       "      <td>org.apache.hadoop.fs.s3a.S3AFileSystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.aws.credentials.provider</th>\n",
       "      <td>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.kubernetes.local.dirs.tmpfs</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              0\n",
       "spark.jars.packages                                          org.apache.hadoop:hadoop-aws:3.2.2\n",
       "spark.kubernetes.namespace                                                                spark\n",
       "spark.kubernetes.container.image                               cronosnull/abd-spark-base:202301\n",
       "spark.executor.instances                                                                      1\n",
       "spark.executor.memory                                                                        1g\n",
       "spark.executor.cores                                                                          1\n",
       "spark.driver.port                                                                         38889\n",
       "spark.driver.blockManager.port                                                             7777\n",
       "spark.driver.bindAddress                                                                0.0.0.0\n",
       "spark.driver.host                                                                  172.24.99.30\n",
       "spark.kubernetes.executor.request.cores                                                    500m\n",
       "spark.driver.memory                                                                          1g\n",
       "spark.hadoop.fs.s3a.endpoint                                           http://172.24.99.18:9000\n",
       "spark.hadoop.fs.s3a.access.key                                                               --\n",
       "spark.hadoop.fs.s3a.secret.key                                                               --\n",
       "spark.hadoop.fs.s3a.path.style.access                                                      True\n",
       "spark.hadoop.fs.s3a.impl                                 org.apache.hadoop.fs.s3a.S3AFileSystem\n",
       "spark.hadoop.fs.s3a.aws.credentials.provider  org.apache.hadoop.fs.s3a.SimpleAWSCredentialsP...\n",
       "spark.kubernetes.local.dirs.tmpfs                                                          True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045fa720",
   "metadata": {},
   "source": [
    "Esto resultó en el siguiente error **OutOfMemoryError** (java.lang.OutOfMemoryError: GC overhead limit exceeded) El cual hace referencía a que el Driver se quedó sin recursos. Por lo cual en la siguiente prueba se intentó aumentar esta capacidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef3d548",
   "metadata": {},
   "source": [
    "### Segunda Prueba:  1 Nodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58df2a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_config = {\n",
    "    \"spark.executor.instances\": \"1\",\n",
    "    \"spark.executor.memory\": \"1g\",\n",
    "    \"spark.executor.cores\": \"1\",\n",
    "    \"spark.driver.memory\":\"4g\",\n",
    "\n",
    "}\n",
    "config = config | second_config\n",
    "config_df = pd.DataFrame.from_dict(config, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e6382a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spark.jars.packages</th>\n",
       "      <td>org.apache.hadoop:hadoop-aws:3.2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.kubernetes.namespace</th>\n",
       "      <td>spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.kubernetes.container.image</th>\n",
       "      <td>cronosnull/abd-spark-base:202301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.executor.instances</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.executor.memory</th>\n",
       "      <td>1g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.executor.cores</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.driver.port</th>\n",
       "      <td>38889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.driver.blockManager.port</th>\n",
       "      <td>7777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.driver.bindAddress</th>\n",
       "      <td>0.0.0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.driver.host</th>\n",
       "      <td>172.24.99.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.kubernetes.executor.request.cores</th>\n",
       "      <td>500m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.driver.memory</th>\n",
       "      <td>4g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.endpoint</th>\n",
       "      <td>http://172.24.99.18:9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.access.key</th>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.secret.key</th>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.path.style.access</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.impl</th>\n",
       "      <td>org.apache.hadoop.fs.s3a.S3AFileSystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.aws.credentials.provider</th>\n",
       "      <td>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.kubernetes.local.dirs.tmpfs</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              0\n",
       "spark.jars.packages                                          org.apache.hadoop:hadoop-aws:3.2.2\n",
       "spark.kubernetes.namespace                                                                spark\n",
       "spark.kubernetes.container.image                               cronosnull/abd-spark-base:202301\n",
       "spark.executor.instances                                                                      1\n",
       "spark.executor.memory                                                                        1g\n",
       "spark.executor.cores                                                                          1\n",
       "spark.driver.port                                                                         38889\n",
       "spark.driver.blockManager.port                                                             7777\n",
       "spark.driver.bindAddress                                                                0.0.0.0\n",
       "spark.driver.host                                                                  172.24.99.30\n",
       "spark.kubernetes.executor.request.cores                                                    500m\n",
       "spark.driver.memory                                                                          4g\n",
       "spark.hadoop.fs.s3a.endpoint                                           http://172.24.99.18:9000\n",
       "spark.hadoop.fs.s3a.access.key                                                               --\n",
       "spark.hadoop.fs.s3a.secret.key                                                               --\n",
       "spark.hadoop.fs.s3a.path.style.access                                                      True\n",
       "spark.hadoop.fs.s3a.impl                                 org.apache.hadoop.fs.s3a.S3AFileSystem\n",
       "spark.hadoop.fs.s3a.aws.credentials.provider  org.apache.hadoop.fs.s3a.SimpleAWSCredentialsP...\n",
       "spark.kubernetes.local.dirs.tmpfs                                                          True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7c4ac5",
   "metadata": {},
   "source": [
    "Con esta configuración se observó que el proceso se terminaba de ejecutar satisfactoríamente. Sin embargo, el tiempo de ejecución fue grande. Este experimento se realizó en la semana 7, por lo tanto, es de esperar que no tantos grupos estaban usando la infraestructura. Por lo tanto podemos asumir que esta demora se debe ahora a que **solo se está usando un Nodo**. Esto no nos permite aprovechar al máximo las ventajas de ejecutar procedimientos de Big Data usando computación distribuida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af17b21",
   "metadata": {},
   "source": [
    "### Tercera Prueba: 4 Nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a856c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_config = {\n",
    "    \"spark.executor.instances\": \"4\",\n",
    "    \"spark.executor.memory\": \"1g\",\n",
    "    \"spark.executor.cores\": \"1\",\n",
    "    \"spark.driver.memory\":\"4g\",\n",
    "\n",
    "}\n",
    "config = config | third_config\n",
    "config_df = pd.DataFrame.from_dict(config, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8661df81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spark.jars.packages</th>\n",
       "      <td>org.apache.hadoop:hadoop-aws:3.2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.kubernetes.namespace</th>\n",
       "      <td>spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.kubernetes.container.image</th>\n",
       "      <td>cronosnull/abd-spark-base:202301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.executor.instances</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.executor.memory</th>\n",
       "      <td>1g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.executor.cores</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.driver.port</th>\n",
       "      <td>38889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.driver.blockManager.port</th>\n",
       "      <td>7777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.driver.bindAddress</th>\n",
       "      <td>0.0.0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.driver.host</th>\n",
       "      <td>172.24.99.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.kubernetes.executor.request.cores</th>\n",
       "      <td>500m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.driver.memory</th>\n",
       "      <td>4g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.endpoint</th>\n",
       "      <td>http://172.24.99.18:9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.access.key</th>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.secret.key</th>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.path.style.access</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.impl</th>\n",
       "      <td>org.apache.hadoop.fs.s3a.S3AFileSystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.hadoop.fs.s3a.aws.credentials.provider</th>\n",
       "      <td>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark.kubernetes.local.dirs.tmpfs</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              0\n",
       "spark.jars.packages                                          org.apache.hadoop:hadoop-aws:3.2.2\n",
       "spark.kubernetes.namespace                                                                spark\n",
       "spark.kubernetes.container.image                               cronosnull/abd-spark-base:202301\n",
       "spark.executor.instances                                                                      4\n",
       "spark.executor.memory                                                                        1g\n",
       "spark.executor.cores                                                                          1\n",
       "spark.driver.port                                                                         38889\n",
       "spark.driver.blockManager.port                                                             7777\n",
       "spark.driver.bindAddress                                                                0.0.0.0\n",
       "spark.driver.host                                                                  172.24.99.30\n",
       "spark.kubernetes.executor.request.cores                                                    500m\n",
       "spark.driver.memory                                                                          4g\n",
       "spark.hadoop.fs.s3a.endpoint                                           http://172.24.99.18:9000\n",
       "spark.hadoop.fs.s3a.access.key                                                               --\n",
       "spark.hadoop.fs.s3a.secret.key                                                               --\n",
       "spark.hadoop.fs.s3a.path.style.access                                                      True\n",
       "spark.hadoop.fs.s3a.impl                                 org.apache.hadoop.fs.s3a.S3AFileSystem\n",
       "spark.hadoop.fs.s3a.aws.credentials.provider  org.apache.hadoop.fs.s3a.SimpleAWSCredentialsP...\n",
       "spark.kubernetes.local.dirs.tmpfs                                                          True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7968fa",
   "metadata": {},
   "source": [
    "**Rendimiento:**\n",
    "\n",
    "Esta configuración (con la que se realizó el ejercicio), presenta una gran mejoría en terminos de eficiencia. Además la implementación de los **Schema** en la lectura de los datos permite tener aún más velocidad. Esto se hizo debido a que los archivos parquet presentan SchemaOnWrite, por lo tanto ya cuentan con una estructura definida al momento de ser guardarlos. El problema es que Spark no sabe si este esquema será el mismo para todos los archivos. Para el caso de las Noticias sí es el caso, por lo tanto podemos aumentar la velocidad leyendo primero un archivo parquet, para extraer su estructura, y luego aplicar esta estructura en la lectura de los demás.\n",
    "\n",
    "Finalmente se decidió usar solo 4 instancías para no acaparar más infraestructura de la necesaria, y no afectar otros grupos. Por esta misma razón siempre se corrío el comando spark.stop para mitigar este problema.\n",
    "\n",
    "**Facilidad de uso**\n",
    "\n",
    "En terminos de usabilidad, sin importar la cantidad de nodos, trabajar en Spark con grandes cantidades de datos puede ser desafiante. Esto se debe porque en el momento de desarrollar es normal presentar errores en el código, los cuales requieren una iteración entre Programar -> Probar -> Arreglar errores. Si se prueba este código con todos los datos desde un inicio, tendremos que esperar una gran cantidad de tiempo para ver los errores que tengamos. \n",
    "\n",
    "Una alternativa es probar nuestro código con una pequeña parte de los datos, esto con el fín de encontrar errores más fácilmente. Ya cuando tengamos una versión lista de nuestro código podemos probarlo ya con la cantidad total de los datos. Sin embargo hay que tener en cuenta que nuestro código debe ser eficiente en terminos de complejidad computacional, podemos crear diversos algoritmos para solucionar un mismo problema, pero no todos serán óptimos cuando la cantidad de datos es muy grande. Por lo tanto puede ocurrir que tengamos un código final que funcione con una porción de los datos, pero al probarlo con todos los datos nos de TimeOut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed52d851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
